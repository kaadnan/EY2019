{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point, LinearRing, box\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('df_train')\n",
    "df_test = pd.read_pickle('df_test')\n",
    "x_train_df = pd.read_pickle('x_train_df')\n",
    "x_test_df = pd.read_pickle('x_test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x_entry = min(min(df_train.x_entry.describe()['min'], df_test.x_entry.describe()['min']),\n",
    "                  df_train.x_exit.describe()['min'])\n",
    "min_y_entry = min(min(df_train.y_entry.describe()['min'], df_test.y_entry.describe()['min']),\n",
    "                  df_train.y_exit.describe()['min'])\n",
    "max_x_entry = max(max(df_train.x_entry.describe()['max'], df_test.x_entry.describe()['max']),\n",
    "                  df_train.x_exit.describe()['max'])\n",
    "max_y_entry = max(max(df_train.y_entry.describe()['max'], df_test.y_entry.describe()['max']),\n",
    "                  df_train.y_exit.describe()['max'])\n",
    "print(min_x_entry, max_x_entry, min_y_entry, max_y_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map():\n",
    "    rect_all = patches.Rectangle((3740998.35481912, -19042656.658487003), (3777099.2656833767 - 3740998.35481912),\n",
    "                             (- 19382914.9809002 + 19042656.658487003),\n",
    "                             linewidth=1,edgecolor='g',fill = False,hatch = '\\\\\\\\\\\\', label = 'city')\n",
    "    city = plt.gca().add_patch(rect_all)\n",
    "    rect_city = patches.Rectangle((3750901.5068, -19208905.6133), (3770901.5068 - 3750901.5068),\n",
    "                             (- 19268905.6133 + 19208905.6133),\n",
    "                             linewidth=1,edgecolor='r',fill = False,hatch = '//////', label = 'city center')\n",
    "    city_center = plt.gca().add_patch(rect_city)\n",
    "    plt.legend(handles=[city, city_center])\n",
    "    plt.grid(True)\n",
    "#     plt.xlim(min_x_entry, max_x_entry)\n",
    "    plt.ylim(min_y_entry, max_y_entry)\n",
    "    plt.xlim(3740000, 3780000)\n",
    "    plt.ylim(-19000000, - 19400000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box(x,y):\n",
    "    n1 = -1\n",
    "    n2 = -1\n",
    "    if(~np.isnan(x) and ~np.isnan(y)):\n",
    "        if(x >= 3740000.0000000000 and x < 3745000.0000000000):\n",
    "            n1 = 1\n",
    "        elif(x >= 3745000.0000000000 and x < 3750000.0000000000):\n",
    "            n1 = 2\n",
    "        elif(x >= 3750000.0000000000 and x < 3755000.0000000000):\n",
    "            n1 = 3\n",
    "        elif(x >= 3755000.0000000000 and x < 3760000.0000000000):\n",
    "            n1 = 4\n",
    "        elif(x >= 3760000.0000000000 and x < 3765000.0000000000):\n",
    "            n1 = 5\n",
    "        elif(x >= 3765000.0000000000 and x < 3770000.0000000000):\n",
    "            n1 = 6\n",
    "        elif(x >= 3770000.0000000000 and x < 3775000.0000000000):\n",
    "            n1 = 7\n",
    "        else:\n",
    "            n1 = 8\n",
    "\n",
    "        if(y >= -19400000.0000000000 and y < -19350000.0000000000):\n",
    "            n2 = 8\n",
    "        elif(y >= -19350000.0000000000 and y < -19300000.0000000000):\n",
    "            n2 = 7\n",
    "        elif(y >= -19300000.0000000000 and y < -19250000.0000000000):\n",
    "            n2 = 6\n",
    "        elif(y >= -19250000.0000000000 and y < -19200000.0000000000):\n",
    "            n2 = 5\n",
    "        elif(y >= -19200000.0000000000 and y < -19150000.0000000000):\n",
    "            n2 = 4\n",
    "        elif(y >= -19150000.0000000000 and y < -19100000.0000000000):\n",
    "            n2 = 3\n",
    "        elif(y >= -19100000.0000000000 and y < -19050000.0000000000):\n",
    "            n2 = 2\n",
    "        else:\n",
    "            n2 = 1\n",
    "    \n",
    "        return (n2-1)*8 + n1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['distance'] = df_test.apply(lambda row: Point(row['x_entry'],row['y_entry']).\n",
    "                                      distance(Point(row['x_exit'],row['y_exit'])) if ~np.isnan(row['x_exit'])\n",
    "                                     else np.nan, axis=1)\n",
    "df_test['velocity'] = df_test.apply(lambda row: (row['distance']/row['duration'])\n",
    "                                      if (row['duration']!= 0.0 and ~np.isnan(row['x_exit'])) else 0.0, axis=1)\n",
    "df_train['entry_box'] = df_train.apply(lambda row: get_box(row['x_entry'], row['y_entry']) , axis=1)\n",
    "df_train['exit_box'] = df_train.apply(lambda row: get_box(row['x_exit'], row['y_exit']) , axis=1)\n",
    "df_test['entry_box'] = df_test.apply(lambda row: get_box(row['x_entry'], row['y_entry']) , axis=1)\n",
    "df_test['exit_box'] = df_test.apply(lambda row: get_box(row['x_exit'], row['y_exit']) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle('df_train')\n",
    "df_test.to_pickle('df_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.groupby('hash')\n",
    "df_list = list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.groupby('hash')\n",
    "df_test_list = list(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "print(len(df_list))\n",
    "for index, sample in enumerate(df_list):\n",
    "    length = sample[1].shape[0]\n",
    "    count = 0\n",
    "    features = {}\n",
    "    features['hash'] = sample[0]\n",
    "    for index1, row in sample[1].iterrows():\n",
    "        features['trajectory_id']  = row['trajectory_id']\n",
    "        if(count < length - 1):\n",
    "            features['entry_'+ str(row['entry_box'])]  = features.get('entry_'+ str(row['entry_box']),0) + 1\n",
    "            features['exit_' + str(row['exit_box'])]  = features.get('exit_'+ str(row['exit_box']),0) + 1\n",
    "        else:\n",
    "            features['l_entry']  = row['entry_box']\n",
    "            features['l_exit']  = row['exit_box']\n",
    "            features['x_entry'] = row['x_entry']\n",
    "            features['y_entry'] = row['y_entry']\n",
    "            features['duration'] = row['duration']\n",
    "            features['city_distance'] = row['city_distance']\n",
    "            features['hour'] = row['hour']\n",
    "            features['label'] = row['label']\n",
    "        count = count + 1\n",
    "    dataset.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "print(len(df_test_list))\n",
    "for index, sample in enumerate(df_test_list):\n",
    "    length = sample[1].shape[0]\n",
    "    count = 0\n",
    "    features = {}\n",
    "    features['hash'] = sample[0]\n",
    "    for index1, row in sample[1].iterrows():\n",
    "        features['trajectory_id']  = row['trajectory_id']\n",
    "        if(count < length - 1):\n",
    "            features['entry_'+ str(row['entry_box'])]  = features.get('entry_'+ str(row['entry_box']),0) + 1\n",
    "            features['exit_' + str(row['exit_box'])]  = features.get('exit_'+ str(row['exit_box']),0) + 1\n",
    "        else:\n",
    "            features['l_entry']  = row['entry_box']\n",
    "            features['l_exit']  = 0\n",
    "            features['x_entry'] = row['x_entry']\n",
    "            features['y_entry'] = row['y_entry']\n",
    "            features['duration'] = row['duration']\n",
    "            features['city_distance'] = row['city_distance']\n",
    "            features['hour'] = row['hour']\n",
    "            features['label'] = -1\n",
    "        count = count + 1\n",
    "    test_dataset.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(dataset).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df = pd.DataFrame(test_dataset).fillna(0)\n",
    "x_test_df['entry_5'] = 0.0\n",
    "x_test_df['exit_5'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_df.shape, x_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df.to_pickle('x_train_df')\n",
    "x_test_df.to_pickle('x_test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = x_train_df.drop(['hash','l_exit','trajectory_id'], axis=1)\n",
    "testing_dataset = x_test_df.drop(['hash','l_exit','trajectory_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = training_dataset.drop(['label'], axis=1).values\n",
    "y_1 = training_dataset['label'].values\n",
    "sc_1 = StandardScaler()\n",
    "x_1 = sc_1.fit_transform(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=60, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(x_1, y_1, test_size = 0.0, random_state = 0)\n",
    "clf_1 = RandomForestClassifier(n_jobs = -1, n_estimators = 500, max_depth = 60, random_state=42)\n",
    "# clf_1 = GaussianNB()\n",
    "# clf_1 = AdaBoostClassifier(n_estimators=100)\n",
    "# clf_1 = SVC(gamma=2, C=1)\n",
    "# clf_1 = MLPClassifier(activation= 'relu',hidden_layer_sizes=(10,10,10))\n",
    "# clf_1 = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "# clf_1 = LogisticRegression(penalty ='l2',dual = True)\n",
    "# clf_1 = DecisionTreeClassifier(max_depth=1000)\n",
    "# clf_1 = KNeighborsClassifier(3)\n",
    "clf_1.fit(xTrain, yTrain)\n",
    "# yPred = clf_1.predict(xTest)\n",
    "# print(classification_report(yTest, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(xTrain, yTrain)\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_features, train_labels)\n",
    "grid_search.best_params_\n",
    "best_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df['label'] =  clf_1.predict(sc_1.fit_transform(testing_dataset.drop(['label'], axis=1).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df['label'] = x_test_df.apply(lambda row: 0 \n",
    "                                 if ((row['duration'] == 0.0) &\n",
    "                                     ~(((row['x_entry'] >= 3750901.5068) &\n",
    "                                                               (row['x_entry'] <= 3770901.5068)) &\n",
    "                                       ((row['y_entry'] >= -19268905.6133) &\n",
    "                                        (row['y_entry'] <= -19208905.6133))))\n",
    "                                 else row['label'], axis=1)\n",
    "x_test_df['label'] = x_test_df.apply(lambda row: 1\n",
    "                                 if ((row['duration'] == 0.0) &\n",
    "                                     (((row['x_entry'] >= 3750901.5068) &\n",
    "                                                               (row['x_entry'] <= 3770901.5068)) &\n",
    "                                       ((row['y_entry'] >= -19268905.6133) &\n",
    "                                        (row['y_entry'] <= -19208905.6133))))\n",
    "                                 else row['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25249\n",
       "1     8266\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df.sort_values('trajectory_id',ascending=True)[['trajectory_id','label']].to_csv('output25.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point, LinearRing, box\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('df_train')\n",
    "df_test = pd.read_pickle('df_test')\n",
    "x_train_df_1 = pd.read_pickle('new_x_train_df')\n",
    "x_test_df_1 = pd.read_pickle('new_x_test_df')\n",
    "x_train_df = pd.read_pickle('x_train_df_2')\n",
    "x_test_df = pd.read_pickle('x_test_df_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = x_train_df.sort_values('hash',ascending=True)\n",
    "x_test_df = x_test_df.sort_values('hash',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.merge(x_train_df, x_train_df_1, on='hash')\n",
    "x_test_df = pd.merge(x_test_df, x_test_df_1, on='hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134063, 155) (33515, 155)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_df.shape, x_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x_entry = min(min(df_train.x_entry.describe()['min'], df_test.x_entry.describe()['min']),\n",
    "                  df_train.x_exit.describe()['min'])\n",
    "min_y_entry = min(min(df_train.y_entry.describe()['min'], df_test.y_entry.describe()['min']),\n",
    "                  df_train.y_exit.describe()['min'])\n",
    "max_x_entry = max(max(df_train.x_entry.describe()['max'], df_test.x_entry.describe()['max']),\n",
    "                  df_train.x_exit.describe()['max'])\n",
    "max_y_entry = max(max(df_train.y_entry.describe()['max'], df_test.y_entry.describe()['max']),\n",
    "                  df_train.y_exit.describe()['max'])\n",
    "print(min_x_entry, max_x_entry, min_y_entry, max_y_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map():\n",
    "    rect_all = patches.Rectangle((3740998.35481912, -19042656.658487003), (3777099.2656833767 - 3740998.35481912),\n",
    "                             (- 19382914.9809002 + 19042656.658487003),\n",
    "                             linewidth=1,edgecolor='g',fill = False,hatch = '\\\\\\\\\\\\', label = 'city')\n",
    "    city = plt.gca().add_patch(rect_all)\n",
    "    rect_city = patches.Rectangle((3750901.5068, -19208905.6133), (3770901.5068 - 3750901.5068),\n",
    "                             (- 19268905.6133 + 19208905.6133),\n",
    "                             linewidth=1,edgecolor='r',fill = False,hatch = '//////', label = 'city center')\n",
    "    city_center = plt.gca().add_patch(rect_city)\n",
    "    plt.legend(handles=[city, city_center])\n",
    "    plt.grid(True)\n",
    "#     plt.xlim(min_x_entry, max_x_entry)\n",
    "    plt.ylim(min_y_entry, max_y_entry)\n",
    "    plt.xlim(3740000, 3780000)\n",
    "    plt.ylim(-19000000, - 19400000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box(x,y):\n",
    "    n1 = -1\n",
    "    n2 = -1\n",
    "    if(~np.isnan(x) and ~np.isnan(y)):\n",
    "        if(x >= 3740000.0000000000 and x < 3745000.0000000000):\n",
    "            n1 = 1\n",
    "        elif(x >= 3745000.0000000000 and x < 3750000.0000000000):\n",
    "            n1 = 2\n",
    "        elif(x >= 3750000.0000000000 and x < 3755000.0000000000):\n",
    "            n1 = 3\n",
    "        elif(x >= 3755000.0000000000 and x < 3760000.0000000000):\n",
    "            n1 = 4\n",
    "        elif(x >= 3760000.0000000000 and x < 3765000.0000000000):\n",
    "            n1 = 5\n",
    "        elif(x >= 3765000.0000000000 and x < 3770000.0000000000):\n",
    "            n1 = 6\n",
    "        elif(x >= 3770000.0000000000 and x < 3775000.0000000000):\n",
    "            n1 = 7\n",
    "        else:\n",
    "            n1 = 8\n",
    "\n",
    "        if(y >= -19400000.0000000000 and y < -19350000.0000000000):\n",
    "            n2 = 8\n",
    "        elif(y >= -19350000.0000000000 and y < -19300000.0000000000):\n",
    "            n2 = 7\n",
    "        elif(y >= -19300000.0000000000 and y < -19250000.0000000000):\n",
    "            n2 = 6\n",
    "        elif(y >= -19250000.0000000000 and y < -19200000.0000000000):\n",
    "            n2 = 5\n",
    "        elif(y >= -19200000.0000000000 and y < -19150000.0000000000):\n",
    "            n2 = 4\n",
    "        elif(y >= -19150000.0000000000 and y < -19100000.0000000000):\n",
    "            n2 = 3\n",
    "        elif(y >= -19100000.0000000000 and y < -19050000.0000000000):\n",
    "            n2 = 2\n",
    "        else:\n",
    "            n2 = 1\n",
    "    \n",
    "        return (n2-1)*8 + n1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['distance'] = df_test.apply(lambda row: Point(row['x_entry'],row['y_entry']).\n",
    "                                      distance(Point(row['x_exit'],row['y_exit'])) if ~np.isnan(row['x_exit'])\n",
    "                                     else np.nan, axis=1)\n",
    "df_test['velocity'] = df_test.apply(lambda row: (row['distance']/row['duration'])\n",
    "                                      if (row['duration']!= 0.0 and ~np.isnan(row['x_exit'])) else 0.0, axis=1)\n",
    "df_train['entry_box'] = df_train.apply(lambda row: get_box(row['x_entry'], row['y_entry']) , axis=1)\n",
    "df_train['exit_box'] = df_train.apply(lambda row: get_box(row['x_exit'], row['y_exit']) , axis=1)\n",
    "df_test['entry_box'] = df_test.apply(lambda row: get_box(row['x_entry'], row['y_entry']) , axis=1)\n",
    "df_test['exit_box'] = df_test.apply(lambda row: get_box(row['x_exit'], row['y_exit']) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle('df_train')\n",
    "df_test.to_pickle('df_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.groupby('hash')\n",
    "df_list = list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_group = df_test.groupby('hash')\n",
    "df_test_list = list(df_test_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134063\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "print(len(df_list))\n",
    "for index, sample in enumerate(df_list):\n",
    "    length = sample[1].shape[0]\n",
    "    count = 0\n",
    "    features = {}\n",
    "    features['hash'] = sample[0]\n",
    "    for index1, row in sample[1].iterrows():\n",
    "        features['trajectory_id']  = row['trajectory_id']\n",
    "        if(count < length - 1):\n",
    "#             features['box_'+ str(row['entry_box'])]  = features.get('box_'+ str(row['entry_box']),0) + 1\n",
    "#             features['box_' + str(row['exit_box'])]  = features.get('box_'+ str(row['exit_box']),0) + 1\n",
    "            features['entry_box_'+ str(row['entry_box'])]  = features.get('entry_box_'+ str(row['entry_box']),0) + 1\n",
    "            features['exit_box_' + str(row['exit_box'])]  = features.get('exit_box_'+ str(row['exit_box']),0) + 1\n",
    "            features['duration_history']  = features.get('duration_history', 0) + row['duration']\n",
    "            features['distance_history']  = features.get('distance_history', 0) + row['distance']\n",
    "#             features['entry_time_'+ str(int(row['time_entry'] / np.timedelta64(1, 'h')))] = features.get('entry_time_'+ str(int(row['time_entry'] / np.timedelta64(1, 'h'))),0) + 1\n",
    "#             features['exit_time_'+ str(int(row['time_exit'] / np.timedelta64(1, 'h')))] = features.get('exit_time_'+ str(int(row['time_entry'] / np.timedelta64(1, 'h'))),0) + 1\n",
    "#             features['box_duration_'+ str(row['entry_box'])]  = features.get('box_duration_'+ str(row['entry_box']),0) + row['duration']\n",
    "#             features['box_distance_'+ str(row['entry_box'])]  = features.get('box_distance'+ str(row['entry_box']),0) + row['distance']\n",
    "            \n",
    "        else:\n",
    "            features['l_entry']  = row['entry_box']\n",
    "#             features['l_exit']  = row['exit_box']\n",
    "            features['x_entry'] = row['x_entry']\n",
    "            features['y_entry'] = row['y_entry']\n",
    "            features['duration'] = row['duration']\n",
    "            features['city_distance'] = row['city_distance']\n",
    "            features['hour'] = row['hour']\n",
    "            features['label'] = row['label']\n",
    "        count = count + 1\n",
    "    features['duration_history'] = features.get('duration_history', 0) / length\n",
    "    features['distance_history'] = features.get('distance_history', 0) / length\n",
    "#     comp = new_train[new_train['hash'] == sample[0]]\n",
    "#     features['avg_azimuth'] = comp.avg_azimuth.mean()\n",
    "#     features['box_area'] = comp.box_area.mean()\n",
    "#     for index2, row in comp.iterrows():\n",
    "#         features['avg_trip_duration_'+ str(row['entry_hour'])]  = features.get('avg_trip_duration_'+ str(row['entry_hour']),0) + row['avg_trip_duration']\n",
    "#         features['avg_trip_length_'+ str(row['entry_hour'])]  = features.get('avg_trip_length_'+ str(row['entry_hour']),0) + row['avg_trip_length']\n",
    "    dataset.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33515\n"
     ]
    }
   ],
   "source": [
    "test_dataset = []\n",
    "print(len(df_test_list))\n",
    "for index, sample in enumerate(df_test_list):\n",
    "    length = sample[1].shape[0]\n",
    "    count = 0\n",
    "    features = {}\n",
    "    features['hash'] = sample[0]\n",
    "    for index1, row in sample[1].iterrows():\n",
    "        features['trajectory_id']  = row['trajectory_id']\n",
    "        if(count < length - 1):\n",
    "#             features['box_'+ str(row['entry_box'])]  = features.get('box_'+ str(row['entry_box']),0) + 1\n",
    "#             features['box_' + str(row['exit_box'])]  = features.get('box_'+ str(row['exit_box']),0) + 1\n",
    "            features['entry_box_'+ str(row['entry_box'])]  = features.get('entry_box_'+ str(row['entry_box']),0) + 1\n",
    "            features['exit_box_' + str(row['exit_box'])]  = features.get('exit_box_'+ str(row['exit_box']),0) + 1\n",
    "            features['duration_history']  = features.get('duration_history', 0) + row['duration']\n",
    "            features['distance_history']  = features.get('distance_history', 0) + row['distance']\n",
    "#             features['entry_time_'+ str(int(row['time_entry'] / np.timedelta64(1, 'h')))] = features.get('entry_time_'+ str(int(row['time_entry'] / np.timedelta64(1, 'h'))),0) + 1\n",
    "#             features['exit_time_'+ str(int(row['time_exit'] / np.timedelta64(1, 'h')))] = features.get('exit_time_'+ str(int(row['time_entry'] / np.timedelta64(1, 'h'))),0) + 1\n",
    "#             features['box_duration_'+ str(row['entry_box'])]  = features.get('box_duration_'+ str(row['entry_box']),0) + row['duration']\n",
    "#             features['box_distance_'+ str(row['entry_box'])]  = features.get('box_distance'+ str(row['entry_box']),0) + row['distance']\n",
    "        else:\n",
    "            features['l_entry']  = row['entry_box']\n",
    "#             features['l_exit']  = 0\n",
    "            features['x_entry'] = row['x_entry']\n",
    "            features['y_entry'] = row['y_entry']\n",
    "            features['duration'] = row['duration']\n",
    "            features['city_distance'] = row['city_distance']\n",
    "            features['hour'] = row['hour']\n",
    "            features['label'] = -1\n",
    "        count = count + 1\n",
    "    features['duration_history'] = features.get('duration_history', 0) / length\n",
    "    features['distance_history'] = features.get('distance_history', 0) / length\n",
    "#     comp = new_test[new_test['hash'] == sample[0]]\n",
    "#     features['avg_azimuth'] = comp.avg_azimuth.mean()\n",
    "#     features['box_area'] = comp.box_area.mean()\n",
    "#     for index2, row in comp.iterrows():\n",
    "#         features['avg_trip_duration_'+ str(row['entry_hour'])]  = features.get('avg_trip_duration_'+ str(row['entry_hour']),0) + row['avg_trip_duration']\n",
    "#         features['avg_trip_length_'+ str(row['entry_hour'])]  = features.get('avg_trip_length_'+ str(row['entry_hour']),0) + row['avg_trip_length']\n",
    "#     if index % 100) == 0:\n",
    "#         print(index)\n",
    "    test_dataset.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(dataset).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df = pd.DataFrame(test_dataset).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_df['box_5'] = 0.0\n",
    "# x_test_df['box_distance_5'] = 0.0\n",
    "# x_test_df['box_duration_5'] = 0.0\n",
    "x_test_df['entry_box_5'] = 0.0\n",
    "x_test_df['exit_box_5'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134063, 155) (33515, 155)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_df.shape, x_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df.to_pickle('x_train_df')\n",
    "x_test_df.to_pickle('x_test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = x_train_df.drop(['hash','trajectory_id'], axis=1)\n",
    "testing_dataset = x_test_df.drop(['hash','trajectory_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = training_dataset.drop(['label'], axis=1).values\n",
    "y_1 = training_dataset['label'].values\n",
    "sc_1 = StandardScaler()\n",
    "x_1 = sc_1.fit_transform(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = decomposition.PCA()\n",
    "# pca.fit(x_1)\n",
    "# plt.figure(1, figsize=(4, 3))\n",
    "# plt.clf()\n",
    "# plt.axes([.2, .2, .7, .7])\n",
    "# plt.plot(pca.explained_variance_, linewidth=2)\n",
    "# plt.axis('tight')\n",
    "# plt.xlabel('n_components')\n",
    "# plt.ylabel('explained_variance_')\n",
    "# x_1 = pca.transform(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      9768\n",
      "           1       0.96      0.87      0.91      3639\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     13407\n",
      "   macro avg       0.96      0.93      0.94     13407\n",
      "weighted avg       0.95      0.95      0.95     13407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(x_1, y_1, test_size = 0.1, random_state = 0)\n",
    "clf_1 = RandomForestClassifier(n_jobs = -1, n_estimators = 1000, max_depth = 100, random_state=42)\n",
    "# clf_1 = GaussianNB()\n",
    "# clf_1 = AdaBoostClassifier(n_estimators=100)\n",
    "# clf_1 = SVC(gamma=2, C=1)\n",
    "# clf_1 = MLPClassifier(activation= 'relu',hidden_layer_sizes=(10,10,10))\n",
    "# clf_1 = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "# clf_1 = LogisticRegression(penalty ='l2',dual = True)\n",
    "# clf_1 = DecisionTreeClassifier(max_depth=1000)\n",
    "# clf_1 = KNeighborsClassifier(3)\n",
    "clf_1.fit(xTrain, yTrain)\n",
    "yPred = clf_1.predict(xTest)\n",
    "print(classification_report(yTest, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 0 (0.309653) city_distance\n",
      "2. feature 115 (0.113286) y_entry\n",
      "3. feature 113 (0.083807) l_entry\n",
      "4. feature 114 (0.046721) x_entry\n",
      "5. feature 141 (0.042615) avg_trip_length_15\n",
      "6. feature 82 (0.039709) exit_box_36\n",
      "7. feature 28 (0.034484) entry_box_36\n",
      "8. feature 2 (0.028980) duration\n",
      "9. feature 140 (0.021608) avg_trip_length_14\n",
      "10. feature 151 (0.021204) box_area\n",
      "11. feature 83 (0.018403) exit_box_37\n",
      "12. feature 29 (0.018118) entry_box_37\n",
      "13. feature 124 (0.016010) avg_trip_duration_15\n",
      "14. feature 112 (0.015939) hour\n",
      "15. feature 1 (0.010523) distance_history\n",
      "16. feature 116 (0.009853) avg_azimuth\n",
      "17. feature 123 (0.009413) avg_trip_duration_14\n",
      "18. feature 3 (0.008560) duration_history\n",
      "19. feature 84 (0.006749) exit_box_38\n",
      "20. feature 30 (0.006726) entry_box_38\n",
      "21. feature 91 (0.004954) exit_box_44\n",
      "22. feature 37 (0.004471) entry_box_44\n",
      "23. feature 139 (0.004073) avg_trip_length_13\n",
      "24. feature 122 (0.003729) avg_trip_duration_13\n",
      "25. feature 138 (0.003592) avg_trip_length_12\n",
      "26. feature 137 (0.003341) avg_trip_length_11\n",
      "27. feature 121 (0.003273) avg_trip_duration_12\n",
      "28. feature 92 (0.003246) exit_box_45\n",
      "29. feature 38 (0.003176) entry_box_45\n",
      "30. feature 120 (0.002998) avg_trip_duration_11\n",
      "31. feature 136 (0.002758) avg_trip_length_10\n",
      "32. feature 119 (0.002707) avg_trip_duration_10\n",
      "33. feature 132 (0.002553) avg_trip_duration_8\n",
      "34. feature 88 (0.002497) exit_box_41\n",
      "35. feature 150 (0.002495) avg_trip_length_9\n",
      "36. feature 149 (0.002495) avg_trip_length_8\n",
      "37. feature 133 (0.002436) avg_trip_duration_9\n",
      "38. feature 148 (0.002420) avg_trip_length_7\n",
      "39. feature 131 (0.002381) avg_trip_duration_7\n",
      "40. feature 89 (0.002350) exit_box_42\n",
      "41. feature 81 (0.002272) exit_box_35\n",
      "42. feature 34 (0.002263) entry_box_41\n",
      "43. feature 27 (0.002068) entry_box_35\n",
      "44. feature 147 (0.001882) avg_trip_length_6\n",
      "45. feature 103 (0.001766) exit_box_55\n",
      "46. feature 130 (0.001750) avg_trip_duration_6\n",
      "47. feature 35 (0.001747) entry_box_42\n",
      "48. feature 49 (0.001677) entry_box_55\n",
      "49. feature 31 (0.001360) entry_box_39\n",
      "50. feature 85 (0.001340) exit_box_39\n",
      "51. feature 73 (0.001291) exit_box_28\n",
      "52. feature 93 (0.001251) exit_box_46\n",
      "53. feature 146 (0.001236) avg_trip_length_5\n",
      "54. feature 42 (0.001230) entry_box_49\n",
      "55. feature 39 (0.001217) entry_box_46\n",
      "56. feature 76 (0.001202) exit_box_30\n",
      "57. feature 22 (0.001194) entry_box_30\n",
      "58. feature 117 (0.001158) avg_trip_duration_0\n",
      "59. feature 36 (0.001156) entry_box_43\n",
      "60. feature 90 (0.001146) exit_box_43\n",
      "61. feature 129 (0.001140) avg_trip_duration_5\n",
      "62. feature 134 (0.001111) avg_trip_length_0\n",
      "63. feature 19 (0.001104) entry_box_28\n",
      "64. feature 67 (0.001091) exit_box_21\n",
      "65. feature 96 (0.001059) exit_box_49\n",
      "66. feature 74 (0.000976) exit_box_29\n",
      "67. feature 33 (0.000968) entry_box_40\n",
      "68. feature 20 (0.000941) entry_box_29\n",
      "69. feature 80 (0.000937) exit_box_34\n",
      "70. feature 13 (0.000917) entry_box_21\n",
      "71. feature 26 (0.000886) entry_box_34\n",
      "72. feature 78 (0.000856) exit_box_32\n",
      "73. feature 25 (0.000830) entry_box_33\n",
      "74. feature 87 (0.000803) exit_box_40\n",
      "75. feature 102 (0.000784) exit_box_54\n",
      "76. feature 46 (0.000760) entry_box_52\n",
      "77. feature 69 (0.000749) exit_box_23\n",
      "78. feature 135 (0.000744) avg_trip_length_1\n",
      "79. feature 48 (0.000727) entry_box_54\n",
      "80. feature 118 (0.000707) avg_trip_duration_1\n",
      "81. feature 100 (0.000702) exit_box_52\n",
      "82. feature 66 (0.000701) exit_box_20\n",
      "83. feature 101 (0.000699) exit_box_53\n",
      "84. feature 6 (0.000697) entry_box_13\n",
      "85. feature 24 (0.000689) entry_box_32\n",
      "86. feature 5 (0.000680) entry_box_12\n",
      "87. feature 94 (0.000680) exit_box_47\n",
      "88. feature 77 (0.000664) exit_box_31\n",
      "89. feature 98 (0.000643) exit_box_50\n",
      "90. feature 12 (0.000643) entry_box_20\n",
      "91. feature 59 (0.000628) exit_box_12\n",
      "92. feature 60 (0.000623) exit_box_13\n",
      "93. feature 15 (0.000622) entry_box_23\n",
      "94. feature 79 (0.000622) exit_box_33\n",
      "95. feature 23 (0.000596) entry_box_31\n",
      "96. feature 47 (0.000590) entry_box_53\n",
      "97. feature 99 (0.000579) exit_box_51\n",
      "98. feature 126 (0.000565) avg_trip_duration_2\n",
      "99. feature 143 (0.000563) avg_trip_length_2\n",
      "100. feature 106 (0.000557) exit_box_58\n",
      "101. feature 40 (0.000545) entry_box_47\n",
      "102. feature 54 (0.000530) entry_box_60\n",
      "103. feature 18 (0.000524) entry_box_27\n",
      "104. feature 72 (0.000518) exit_box_27\n",
      "105. feature 11 (0.000516) entry_box_19\n",
      "106. feature 45 (0.000504) entry_box_51\n",
      "107. feature 128 (0.000504) avg_trip_duration_4\n",
      "108. feature 145 (0.000497) avg_trip_length_4\n",
      "109. feature 65 (0.000489) exit_box_19\n",
      "110. feature 8 (0.000489) entry_box_15\n",
      "111. feature 144 (0.000483) avg_trip_length_3\n",
      "112. feature 44 (0.000473) entry_box_50\n",
      "113. feature 4 (0.000471) entry_box_11\n",
      "114. feature 127 (0.000470) avg_trip_duration_3\n",
      "115. feature 14 (0.000467) entry_box_22\n",
      "116. feature 68 (0.000466) exit_box_22\n",
      "117. feature 52 (0.000464) entry_box_58\n",
      "118. feature 108 (0.000443) exit_box_60\n",
      "119. feature 61 (0.000440) exit_box_14\n",
      "120. feature 7 (0.000429) entry_box_14\n",
      "121. feature 109 (0.000400) exit_box_61\n",
      "122. feature 62 (0.000390) exit_box_15\n",
      "123. feature 107 (0.000383) exit_box_59\n",
      "124. feature 58 (0.000381) exit_box_11\n",
      "125. feature 53 (0.000379) entry_box_59\n",
      "126. feature 55 (0.000306) entry_box_61\n",
      "127. feature 56 (0.000288) entry_box_62\n",
      "128. feature 16 (0.000284) entry_box_24\n",
      "129. feature 110 (0.000275) exit_box_62\n",
      "130. feature 17 (0.000273) entry_box_26\n",
      "131. feature 41 (0.000258) entry_box_48\n",
      "132. feature 95 (0.000253) exit_box_48\n",
      "133. feature 70 (0.000239) exit_box_24\n",
      "134. feature 71 (0.000210) exit_box_26\n",
      "135. feature 51 (0.000127) entry_box_57\n",
      "136. feature 105 (0.000105) exit_box_57\n",
      "137. feature 32 (0.000093) entry_box_4\n",
      "138. feature 10 (0.000083) entry_box_18\n",
      "139. feature 86 (0.000055) exit_box_4\n",
      "140. feature 64 (0.000055) exit_box_18\n",
      "141. feature 9 (0.000030) entry_box_16\n",
      "142. feature 63 (0.000027) exit_box_16\n",
      "143. feature 50 (0.000008) entry_box_56\n",
      "144. feature 75 (0.000008) exit_box_3\n",
      "145. feature 111 (0.000008) exit_box_63\n",
      "146. feature 57 (0.000007) entry_box_63\n",
      "147. feature 21 (0.000007) entry_box_3\n",
      "148. feature 97 (0.000001) exit_box_5\n",
      "149. feature 104 (0.000001) exit_box_56\n",
      "150. feature 43 (0.000000) entry_box_5\n",
      "151. feature 125 (0.000000) avg_trip_duration_16\n",
      "152. feature 142 (0.000000) avg_trip_length_16\n"
     ]
    }
   ],
   "source": [
    "features = training_dataset.drop(['label'], axis=1).columns.values\n",
    "importances = clf_1.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "# for f in range(xTrain.shape[1]):\n",
    "#     print(f + 1, importances[f], features[f])\n",
    "for f in range(xTrain.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]), features[indices[f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(xTrain, yTrain)\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_features, train_labels)\n",
    "grid_search.best_params_\n",
    "best_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df['label'] =  clf_1.predict(sc_1.fit_transform(testing_dataset.drop(['label'], axis=1).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df['label'] = x_test_df.apply(lambda row: 0 \n",
    "                                 if ((row['duration'] == 0.0) &\n",
    "                                     ~(((row['x_entry'] >= 3750901.5068) &\n",
    "                                                               (row['x_entry'] <= 3770901.5068)) &\n",
    "                                       ((row['y_entry'] >= -19268905.6133) &\n",
    "                                        (row['y_entry'] <= -19208905.6133))))\n",
    "                                 else row['label'], axis=1)\n",
    "x_test_df['label'] = x_test_df.apply(lambda row: 1\n",
    "                                 if ((row['duration'] == 0.0) &\n",
    "                                     (((row['x_entry'] >= 3750901.5068) &\n",
    "                                                               (row['x_entry'] <= 3770901.5068)) &\n",
    "                                       ((row['y_entry'] >= -19268905.6133) &\n",
    "                                        (row['y_entry'] <= -19208905.6133))))\n",
    "                                 else row['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25171\n",
       "1     8344\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df.sort_values('trajectory_id',ascending=True)[['trajectory_id','label']].to_csv('output35.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_not_in_city = x_test_df[~(((x_test_df['x_entry'] >= 3750901.5068) &\n",
    "         (x_test_df['x_entry'] <= 3770901.5068)) & ((x_test_df['y_entry'] >= -19268905.6133) &\n",
    "         (x_test_df['y_entry'] <= -19208905.6133)))]\n",
    "test_start_in_city = x_test_df[((x_test_df['x_entry'] >= 3750901.5068) &\n",
    "         (x_test_df['x_entry'] <= 3770901.5068)) & ((x_test_df['y_entry'] >= -19268905.6133) &\n",
    "         (x_test_df['y_entry'] <= -19208905.6133))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23951\n",
       "1       20\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_start_not_in_city.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_not_in_city.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_in_city.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_in_city.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_row(row):\n",
    "    rect = patches.Rectangle((3750901.5068,-19208905.6133), (3770901.5068 - 3750901.5068),\n",
    "                             (- 19268905.6133 + 19208905.6133),\n",
    "                             linewidth=1,edgecolor='g',fill = False, label = 'city center')\n",
    "    city = plt.gca().add_patch(rect)\n",
    "    arrows = plt.plot([row['x_entry'], row['x_exit']], [row['y_entry'], row['y_exit']], color = 'b')\n",
    "    entries = plt.scatter(row['x_entry'],row['y_entry'], label='Entry point', color = 'b')\n",
    "    exits = plt.scatter(row['x_exit'],row['y_exit'], label='Exit point', color = 'r')\n",
    "    plt.legend(handles=[city,entries, exits])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_row(df_test[df_test['hash'] == '726ca607cd4197a8e33eafc8de6862bf_23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = pd.read_csv(\"data_avg.csv\")\n",
    "new_test = pd.read_csv(\"data_test_avg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = new_train.groupby('hash')\n",
    "new_train_df_list = list(new_train_df)\n",
    "new_test_df = new_test.groupby('hash')\n",
    "new_test_df_list = list(new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1 = []\n",
    "print(len(new_train_df_list))\n",
    "for index, sample in enumerate(new_train_df_list):\n",
    "    features = {}\n",
    "    features['hash'] = sample[0]\n",
    "    for index1, row in sample[1].iterrows():\n",
    "        features['avg_trip_duration_'+ str(row['entry_hour'])]  = features.get('avg_trip_duration_'+ str(row['entry_hour']),0) + row['avg_trip_duration']\n",
    "        features['avg_trip_length_'+ str(row['entry_hour'])]  = features.get('avg_trip_length_'+ str(row['entry_hour']),0) + row['avg_trip_length']\n",
    "    features['avg_azimuth'] = sample[1].avg_azimuth.mean()\n",
    "    features['box_area'] = sample[1].box_area.mean()\n",
    "    train_dataset_1.append(features)\n",
    "    if index % 10000 == 0:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_1 = []\n",
    "print(len(new_test_df_list))\n",
    "for index, sample in enumerate(new_test_df_list):\n",
    "    features = {}\n",
    "    features['hash'] = sample[0]\n",
    "    for index1, row in sample[1].iterrows():\n",
    "        features['avg_trip_duration_'+ str(row['entry_hour'])]  = features.get('avg_trip_duration_'+ str(row['entry_hour']),0) + row['avg_trip_duration']\n",
    "        features['avg_trip_length_'+ str(row['entry_hour'])]  = features.get('avg_trip_length_'+ str(row['entry_hour']),0) + row['avg_trip_length']\n",
    "    features['avg_azimuth'] = sample[1].avg_azimuth.mean()\n",
    "    features['box_area'] = sample[1].box_area.mean()\n",
    "    test_dataset_1.append(features)\n",
    "    if index % 10000 == 0:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df_1 = pd.DataFrame(train_dataset_1).fillna(0)\n",
    "x_test_df_1 = pd.DataFrame(test_dataset_1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df_1 = x_train_df_1.sort_values('hash',ascending=True)\n",
    "x_test_df_1 = x_test_df_1.sort_values('hash',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_df_1.shape, x_test_df_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df_1.to_pickle('new_x_train_df')\n",
    "x_test_df_1.to_pickle('new_x_test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
